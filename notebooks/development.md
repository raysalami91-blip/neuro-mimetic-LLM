# Week 1: Foundation
tasks_week1 = """
1. Set up Python environment with minimal dependencies
2. Fine-tune GPT-2 Small on English Wikipedia (1GB)
3. Implement basic multi-scale tokenizer interface
4. Create FastAPI skeleton with test endpoints
"""

# Week 2: Core Architecture
tasks_week2 = """
1. Implement minimal neuro-mimetic model (6 layers, 768 dim)
2. Add plastic attention mechanism
3. Create working/long-term memory separation
4. Test with small context (512 tokens)
"""

# Week 3: Training Pipeline
tasks_week3 = """
1. Implement curriculum learning (start 512 â†’ 4096 tokens)
2. Add slow plasticity constraints
3. Create evaluation metrics for lifelong learning
4. Train on OpenWebText (small subset)
"""

# Week 4: Arabic Tokenizer (Isolated)
tasks_week4 = """
1. Build Arabic root extractor (heuristic-based)
2. Implement pattern recognition for common forms
3. Create compression metrics vs standard tokenizers
4. Test tokenizer API separately
"""

# Week 5: Integration & Testing
tasks_week5 = """
1. Integrate Arabic tokenizer with model
2. Test on Arabic Wikipedia (100MB)
3. Implement context growth mechanism
4. Create demo website for remote testing
"""

# Week 6: Optimization & Scale
tasks_week6 = """
1. Optimize memory usage
2. Implement gradient checkpointing
3. Scale context to 8192 tokens
4. Deploy on free cloud (Hugging Face Spaces, Replit)
"""